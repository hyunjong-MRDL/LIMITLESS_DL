{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm, torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RES = timm.create_model(\"resnet50\", pretrained=True)\n",
    "VGG = timm.create_model(\"vgg16_bn\", pretrained=True)\n",
    "INC = timm.create_model(\"inception_v3\", pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RES.fc = nn.Linear(in_features=2048, out_features=2, bias=True)\n",
    "VGG.head.fc = nn.Linear(in_features=4096, out_features=2, bias=True)\n",
    "INC.fc = nn.Linear(in_features=2048, out_features=2, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC00\\anaconda3\\envs\\CAM\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC00\\anaconda3\\envs\\CAM\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "WARNING:root:no value was provided for `target_layer`, thus set to 'layer4'.\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet18\n",
    "model = resnet18(pretrained=True).eval()\n",
    "\n",
    "from torchcam.methods import SmoothGradCAMpp\n",
    "cam_extractor = SmoothGradCAMpp(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC00\\anaconda3\\envs\\CAM\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC00\\anaconda3\\envs\\CAM\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "WARNING:root:no value was provided for `target_layer`, thus set to 'layer4'.\n"
     ]
    }
   ],
   "source": [
    "from torchvision.io.image import read_image\n",
    "from torchvision.transforms.functional import normalize, resize, to_pil_image\n",
    "from torchvision.models import resnet18\n",
    "from torchcam.methods import SmoothGradCAMpp\n",
    "\n",
    "model = resnet18(pretrained=True).eval()\n",
    "target_layer = model.layer4[2].conv3\n",
    "# Get your input\n",
    "img = read_image(\"D:/Datasets/SJS/Processed/SJS/PTG_46512-0000.jpg\")\n",
    "# Preprocess it for your chosen model\n",
    "input_tensor = normalize(resize(img, (224, 224)) / 255., [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "with SmoothGradCAMpp(model, target_layer) as cam_extractor:\n",
    "  # Preprocess your data and feed it to the model\n",
    "  out = model(input_tensor.unsqueeze(0))\n",
    "  # Retrieve the CAM by passing the class index and the model output\n",
    "  activation_map = cam_extractor(out.squeeze(0).argmax().item(), out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Visualize the raw CAM\n",
    "plt.imshow(activation_map[0].squeeze(0).numpy()); plt.axis('off'); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchcam.utils import overlay_mask\n",
    "\n",
    "def save_cam(image):\n",
    "    result = overlay_mask(to_pil_image(image), to_pil_image(activation_map[0].squeeze(0), mode='F'), alpha=0.5)\n",
    "    plt.imshow(result); plt.axis('off'); plt.tight_layout(); plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC00\\anaconda3\\envs\\CAM\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC00\\anaconda3\\envs\\CAM\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import utils, data, models, PIL\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchcam.methods import SmoothGradCAMpp\n",
    "from torchcam.utils import overlay_mask\n",
    "\n",
    "control = \"NONSJS\"\n",
    "\n",
    "model = models.ResNet(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "target_layer = model.layer4[2].conv3\n",
    "\n",
    "max_images = data.read_max_img_paths(f\"C:/Users/PC00/Desktop/SJS/{control}_max_output_images.txt\")\n",
    "for image in max_images:\n",
    "    diagnosis = image.split(\"/\")[4]\n",
    "    gland = image.split(\"/\")[-1].split(\"_\")[0]\n",
    "    ID = image.split(\"/\")[-1].split(\"_\")[1].split(\"-\")[0]\n",
    "    \n",
    "    save_folder = f\"E:/Results/SJS/Figures/CAM/Trained/{control}_SJS/{diagnosis}/\"\n",
    "    \n",
    "    image_name = f\"{save_folder}{ID}_{gland}.jpg\"\n",
    "\n",
    "    \"\"\"(1) Transform the image and put it into the model.\"\"\"\n",
    "    transformed = utils.preprocessing(PIL.Image.open(image)).float().unsqueeze(0)\n",
    "    \n",
    "    with SmoothGradCAMpp(model, target_layer) as cam_extractor:\n",
    "        out = model(transformed)\n",
    "        activation_map = cam_extractor(out.squeeze(0).argmax().item(), out)\n",
    "    \n",
    "    result = overlay_mask(PIL.Image.open(image), to_pil_image(activation_map[0].squeeze(0), mode='F'), alpha=0.5)\n",
    "    plt.figure(); plt.imshow(result); plt.axis('off'); plt.tight_layout(); plt.savefig(image_name); plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import pandas as pd\n",
    "\n",
    "control_group = \"NONSJS\"\n",
    "excel_path = f\"E:\\\\LIMITLESS_DL\\\\SJS\\\\Correct_IDs({control_group}).xlsx\"\n",
    "df = pd.read_excel(excel_path)\n",
    "total_paths = df[\"Path\"]\n",
    "total_classes = df[\"Class\"]\n",
    "\n",
    "data_root = \"D:\\\\Datasets\\\\SJS\\\\Processed\\\\\"\n",
    "\n",
    "dst_root = \"C:\\\\Users\\\\PC00\\\\Desktop\\\\GradCAM\\\\\"\n",
    "control_root = os.path.join(dst_root, f\"{control_group}_SJS\")\n",
    "os.makedirs(os.path.join(control_root, \"PTG\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(control_root, \"SMG\"), exist_ok=True)\n",
    "\n",
    "for path in total_paths:\n",
    "    filename = os.path.basename(path)\n",
    "    curr_ID = filename.split(\"_\")[1].split(\"-\")[0]\n",
    "    curr_gland = filename.split(\"_\")[0]\n",
    "    curr_class = path.split(\"\\\\\")[4]\n",
    "    for img in os.listdir(os.path.join(data_root, curr_class)):\n",
    "        if curr_ID in img:\n",
    "            src_path = os.path.join(data_root, curr_class, img)\n",
    "            break\n",
    "    dst_path = os.path.join(control_root, curr_gland, f\"{curr_ID}_{curr_gland}.jpg\")\n",
    "    shutil.copy(src_path, dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, data\n",
    "import pandas as pd\n",
    "\n",
    "data_root = \"D:\\\\Datasets\\\\SJS\\\\Processed\\\\\"\n",
    "control_group = \"NONSJS\"\n",
    "SJS_path = data.path_by_diagnosis(data_root, \"SJS\")\n",
    "CTR_path = data.path_by_diagnosis(data_root, control_group)\n",
    "SJS_ID = data.ID_summary(SJS_path)\n",
    "CTR_ID = data.ID_summary(CTR_path)\n",
    "\n",
    "result_root = \"C:\\\\Users\\\\PC00\\\\Desktop\\\\GradCAM\\\\\"\n",
    "control_root = os.path.join(result_root, f\"{control_group}_SJS\")\n",
    "\n",
    "PG_df, SG_df = dict(), dict()\n",
    "PG_IDs, PG_classes = [], []\n",
    "SG_IDs, SG_classes = [], []\n",
    "for gland in os.listdir(control_root):\n",
    "    gland_dir = os.path.join(control_root, gland)\n",
    "    for filename in os.listdir(gland_dir):\n",
    "        curr_ID = filename.split(\"_\")[0]\n",
    "        if curr_ID in SJS_ID: curr_class = \"SJS\"\n",
    "        else: curr_class = control_group\n",
    "        if gland == \"PTG\":\n",
    "            PG_IDs.append(curr_ID)\n",
    "            PG_classes.append(curr_class)\n",
    "        else:\n",
    "            SG_IDs.append(curr_ID)\n",
    "            SG_classes.append(curr_class)\n",
    "PG_df[\"ID\"] = PG_IDs\n",
    "PG_df[\"Class\"] = PG_classes\n",
    "SG_df[\"ID\"] = SG_IDs\n",
    "SG_df[\"Class\"] = SG_classes\n",
    "pd.DataFrame(PG_df).to_excel(os.path.join(control_root, \"PTG.xlsx\"))\n",
    "pd.DataFrame(SG_df).to_excel(os.path.join(control_root, \"SMG.xlsx\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
